/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.pig.backend.hadoop.executionengine.spark.plan;

import java.util.BitSet;
import java.util.HashSet;
import java.util.Set;

import org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator;
import org.apache.pig.backend.hadoop.executionengine.physicalLayer.plans.PhysicalPlan;
import org.apache.pig.impl.plan.Operator;
import org.apache.pig.impl.plan.OperatorKey;
import org.apache.pig.impl.plan.VisitorException;

/**
 * An operator model for a Spark job. Acts as a host to the plans that will
 * execute in spark.
 */
public class SparkOperator extends Operator<SparkOpPlanVisitor> {
    private static enum OPER_FEATURE {
        NONE,
        // Indicate if this job is a sampling job
        SAMPLER,
        // Indicate if this job is a merge indexer
        INDEXER,
        // Indicate if this job is a group by job
        GROUPBY,
        // Indicate if this job is a cogroup job
        COGROUP,
        // Indicate if this job is a regular join job
        HASHJOIN,
        // Indicate if this job is a sample aggregation job
        SAMPLE_AGGREGATOR,
        // Indicate if this job is a sample based partition job (order by/skewed join)
        SAMPLE_BASED_PARTITIONER,
        // Indicate if this job is a limit job after sort
        LIMIT_AFTER_SORT;
    }

    public PhysicalPlan physicalPlan;

    public Set<String> UDFs;

    /* Name of the Custom Partitioner used */
    public String customPartitioner = null;

    public Set<PhysicalOperator> scalars;

    public boolean isUDFComparatorUsed = false;

    public int requestedParallelism = -1;

    //private OPER_FEATURE feature = OPER_FEATURE.NONE;
    private BitSet feature = new BitSet();

    private boolean splitter = false;

    // Name of the partition file generated by sampling process,
    // Used by Skewed Join
    private String skewedJoinPartitionFile;

    private boolean usingTypedComparator = false;

    private boolean combineSmallSplits = true;

    // Do not estimate parallelism for specific vertices like limit, indexer,
    // etc which should always be one
    private boolean dontEstimateParallelism = false;

    //Indicates if this job is an order by job
    boolean globalSort = false;

    long limit = -1;

    //The sort order of the columns;
    //asc is true and desc is false
    boolean[] sortOrder;

    private boolean useMRMapSettings = false;

    // Used by sample vertex, send parallelism event to orderOperator
    private SparkOperator sortOperator = null;

    private boolean needEstimateParallelism = false;

    public SparkOperator(OperatorKey k) {
        super(k);
        physicalPlan = new PhysicalPlan();
        UDFs = new HashSet<String>();
        scalars = new HashSet<PhysicalOperator>();
    }

//    public SparkOperator getSortOperator() {
//        return sortOperator;
//    }
//
//    public void setSortOperator(SparkOperator sortOperator) {
//        this.sortOperator = sortOperator;
//    }


    public boolean isGlobalSort() {
        return globalSort;
    }

    public void setGlobalSort() {
        this.globalSort = true;
    }


    public void setSortOrder(boolean[] sortOrder) {
        if (null == sortOrder) return;
        this.sortOrder = new boolean[sortOrder.length];
        for (int i = 0; i < sortOrder.length; ++i) {
            this.sortOrder[i] = sortOrder[i];
        }
    }

    @Override
    public boolean supportsMultipleInputs() {
        return true;
    }

    @Override
    public boolean supportsMultipleOutputs() {
        return true;
    }

    @Override
    public String name() {
        String udfStr = getUDFsAsStr();
        StringBuilder sb = new StringBuilder("Spark" + "("
                + requestedParallelism + (udfStr.equals("") ? "" : ",")
                + udfStr + ")" + " - " + mKey.toString());
        return sb.toString();
    }

    private String getUDFsAsStr() {
        StringBuilder sb = new StringBuilder();
        if (UDFs != null && UDFs.size() > 0) {
            for (String str : UDFs) {
                sb.append(str.substring(str.lastIndexOf('.') + 1));
                sb.append(',');
            }
            sb.deleteCharAt(sb.length() - 1);
        }
        return sb.toString();
    }

    public void add(PhysicalOperator physicalOper) {
        this.physicalPlan.add(physicalOper);
    }

    @Override
    public void visit(SparkOpPlanVisitor v) throws VisitorException {
        v.visitSparkOp(this);
    }

    public boolean isGroupBy() {
        return feature.get(OPER_FEATURE.GROUPBY.ordinal());
    }

    public void markGroupBy() {
        feature.set(OPER_FEATURE.GROUPBY.ordinal());
    }

    public boolean isCogroup() {
        return feature.get(OPER_FEATURE.COGROUP.ordinal());
    }

    public void markCogroup() {
       feature.set(OPER_FEATURE.COGROUP.ordinal());
    }

    public boolean isRegularJoin() {
        return feature.get(OPER_FEATURE.HASHJOIN.ordinal());
    }

    public void markRegularJoin() {
       feature.set(OPER_FEATURE.HASHJOIN.ordinal());
    }

    public int getRequestedParallelism() {
        return requestedParallelism;
    }

    public void setSplitter(boolean spl) {
        splitter = spl;
    }

    public boolean isSplitter() {
        return splitter;
    }

    public boolean isSampler() {
        return feature.get(OPER_FEATURE.SAMPLER.ordinal());
    }

    public void markSampler() {
        feature.set(OPER_FEATURE.SAMPLER.ordinal());
    }

    public void setSkewedJoinPartitionFile(String file) {
        skewedJoinPartitionFile = file;
    }

    public String getSkewedJoinPartitionFile() {
        return skewedJoinPartitionFile;
    }

    protected boolean usingTypedComparator() {
        return usingTypedComparator;
    }

    protected void useTypedComparator(boolean useTypedComparator) {
        this.usingTypedComparator = useTypedComparator;
    }

    protected void noCombineSmallSplits() {
        combineSmallSplits = false;
    }

    public boolean combineSmallSplits() {
        return combineSmallSplits;
    }

    public boolean isIndexer() {
        return feature.get(OPER_FEATURE.INDEXER.ordinal());
    }

    public void markIndexer() {
        feature.set(OPER_FEATURE.INDEXER.ordinal());
    }

    public boolean isLimitAfterSort() {
        return feature.get(OPER_FEATURE.LIMIT_AFTER_SORT.ordinal());
    }

    public void markLimitAfterSort() {
        feature.set(OPER_FEATURE.LIMIT_AFTER_SORT.ordinal());
    }

    public boolean isUseMRMapSettings() {
        return useMRMapSettings;
    }

    public void setUseMRMapSettings(boolean useMRMapSettings) {
        this.useMRMapSettings = useMRMapSettings;
    }

    public void setNeedEstimatedQuantile(boolean needEstimateParallelism) {
        this.needEstimateParallelism = needEstimateParallelism;
    }

    public boolean isNeedEstimateParallelism() {
        return needEstimateParallelism;
    }

    public boolean isDontEstimateParallelism() {
        return dontEstimateParallelism;
    }

    public void setDontEstimateParallelism(boolean dontEstimateParallelism) {
        this.dontEstimateParallelism = dontEstimateParallelism;
    }

    public boolean isSampleAggregation() {
        return feature.get(OPER_FEATURE.SAMPLE_AGGREGATOR.ordinal());
    }

    public void markSampleAggregation() {
        feature.set(OPER_FEATURE.SAMPLE_AGGREGATOR.ordinal());
    }

    public boolean isSampleBasedPartitioner() {
        return feature.get(OPER_FEATURE.SAMPLE_BASED_PARTITIONER.ordinal());
    }

    public void markSampleBasedPartitioner() {
        feature.set(OPER_FEATURE.SAMPLE_BASED_PARTITIONER.ordinal());
    }

}
